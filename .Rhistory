library(lme4)
df <- read.csv('./204_assignment_brendel.csv', header = TRUE)
attach(df)
summary(df)
library(lme4)
df <- read.csv('./204_assignment_brendel.csv', header = TRUE)
attach(df)
## Exploration
summary(df)
library(lme4)
df <- read.csv('./204_assignment_brendel.csv', header = TRUE)
attach(df)
## Exploration
summary(df)
hist(SA_POS)
plot(SA_POS)
library(lme4)
df <- read.csv('./204_assignment_brendel.csv', header = TRUE)
attach(df)
## Exploration
summary(df)
plot(SA_POS)
# Many more instances of SA_POS = BEFORE than = AFTER. This is consistent withplot(SA_POS ~ CENT)
plot(SA_POS ~ CENT)
plot(SA_POS ~ CENT)
summary(df)
rm(list=ls(all=TRUE))
rm(list=ls(all=TRUE))
library(lme4)
df <- read.csv('./204_assignment_brendel.csv', header = TRUE)
df$CENT <- factor(df$CENT)
attach(df)
## Exploration
summary(df)
plot(SA_POS ~ CENT)
plot(SA_POS ~ RC)
plot(SA_POS ~ HAS_RC)
plot(SA_POS ~ TEXT)
plot(TEXT)
hist(TEXT)
plot(TEXT)
plot(SA_POS ~ TEXT)
plot(SA_POS ~ TEXT)
plot(TEXT)
plot(SA_POS ~ TEXT)
m.1 <- lmer(SA_POS~1+HAS_RC+(1|CENT/TEXT), data=df)
m.1 <- glmer(SA_POS~1+HAS_RC+(1|CENT/TEXT), data=df)
m.1 <- glmer(SA_POS~1+HAS_RC+(1|CENT/TEXT), data=df, family="binomial")
summary(m.1)
m.1 <- glmer(SA_POS~1+HAS_RC+(1|CENT/TEXT), data=df, family="binomial", correlation= FALSE)
summary(m.1, correlation=FALSE)
LEVEL1 <- CENT
LEVEL2 <- CENT:TEXT
m.1 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial")
summary(m.1, correlation=FALSE)
# Can CENT be deleted?
m.2 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL2), data=df, family="binomial")
anova(m.1, m.2)
m.3 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL1), data=df, family="binomial")
anova(m.1, m.3)
abs(5-5)
abs(5-6)
abs(-6-5)
abs(-6+5)
abs(5-6)
abs(6-5)
rm(list=ls(all=TRUE))
library(lme4)
df <- read.csv("./204_assignment_brendel.csv", header = TRUE)
df$CENT <- factor(df$CENT)
attach(df)
## Exploration
summary(df)
hist(SA_SEP)
hist(log(SA_SEP))
hist(SA_SEP)
hist(log1p(SA_SEP))
(qwe <- powerTransform(df$SA_SEP ~ 1, family="bcnPower"))
hist(bcnPower(df$SA_SEP, lambda=qwe[[1]], gamma=qwe[[2]]))
rm(list=ls(all=TRUE))
library(lme4)
df <- read.csv("./204_assignment_brendel.csv", header = TRUE)
df$CENT <- factor(df$CENT)
## Exploration
summary(df)
hist(df$SA_SEP)
# Most are adjacent...
hist(df$SA_SEP)
(qwe <- powerTransform(df$SA_SEP ~ 1, family="bcnPower"))
hist(bcnPower(df$SA_SEP, lambda=qwe[[1]], gamma=qwe[[2]]))library(MuMIn)
library(MuMIn)
install.packages(MuMIn)
install.packages('MuMIn')
hist(sqrt(df$SA_SEP))
hist(df$SA_SEP)
hist(sqrt(df$SA_SEP))
hist(sign(df$SA_SEP) * abs(df$SA_SEP)^(1/3))
hist(sign(df$SA_SEP) * abs(df$SA_SEP)^(1/3))
hist(log(df$SA_SEP))
plot(df$SA_POS ~ df$CENT)
boxplot(df$SA_POS ~ df$CENT)
boxplot(df$SA_POS ~ df$CENT)
boxplot(df$SA_SEP ~ df$CENT)
hist(df$SA_SEP)
LEVEL1 <- df$CENT
LEVEL2 <- df$CENT:df$TEXT
m.1 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial")
summary(m.1, correlation=FALSE)
# Can CENT be deleted?
m.2 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL2), data=df, family="binomial")
anova(m.1, m.2)
# No; significant difference
# Can CENT/TEXT be deleted?
m.3 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1), data=df, family="binomial")
anova(m.1, m.3)
# No.
drop1(m.1, test="Chisq")
plot(residuals(m.1))
install.packages("MuMIn")
rm(list=ls(all=TRUE))
library(lme4)
library(MuMIn)
df <- read.csv("./204_assignment_brendel.csv", header = TRUE)
df$CENT <- factor(df$CENT)
## Exploration
summary(df)
hist(df$SA_SEP)
# Most are adjacent...
hist(sqrt(df$SA_SEP))
hist(sign(df$SA_SEP) * abs(df$SA_SEP)^(1/3))
hist(log(df$SA_SEP))
# These all are terrible. I can't imagine this will be a very informative factor.
plot(df$SA_POS)
# Many more instances of SA_POS = BEFORE than AFTER. This is consistent with Sapp (2019)'s claims that prenominal s치 is more frequent overall.
plot(df$SA_POS ~ df$CENT)
# Frequency of SA_POS = AFTER is much higher up to and including 15th century texts, whereafter it is consistently lower (except for potential outliers in 19th century?).
plot(df$SA_POS ~ df$HAS_RC)
# This looks like a fairly clear preference for SA_POS = AFTER when the NP contains a dependent relative clause, which seems to be what we'd expect.
plot(df$TEXT)
plot(df$SA_POS ~ df$TEXT)
# It looks like some texts contribute many more cases of an RC with s치 than other texts. Additionally, while most texts show SA_POS = AFTER is more frequent, they seem to differ greatly in the starkness of the difference in frequency between the two levels.
LEVEL1 <- df$CENT
LEVEL2 <- df$CENT:df$TEXT
m.1 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial")
summary(m.1, correlation=FALSE)
# Can CENT be deleted?
m.2 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL2), data=df, family="binomial")
anova(m.1, m.2)
# No; significant difference
# Can CENT/TEXT be deleted?
m.3 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1), data=df, family="binomial")
anova(m.1, m.3)
# No.
# 
drop1(m.1, test="Chisq")
library(MuMIn)
MuMIn::r.squaredGLMM(m.1)
m.final <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|CENT/TEXT), data=df, family="binomial")
summary(m.1)
summary(m.final)
m.1r <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|CENT/TEXT), data=df, family="binomial")
drop1(m.1r, test="Chisq")
m.final <- m.1r
MuMIn::r.squaredGLMM(m.final)
model.null <- glmer(SA_POS ~ 1 + (1|CENT/TEXT), family=binomial, data=df)
anova(model.final, model.null, test="Chisq") # LR chi-squared=786.86, df=6, p<0.0001
m.null <- glmer(SA_POS ~ 1 + (1|CENT/TEXT), family=binomial, data=df)
anova(m.final, m.null, test="Chisq") # LR chi-squared=786.86, df=6, p<0.0001
LMERConvenienceFunctions::relLik(m.final, m.null)
library(car); library(effects); library(lme4); library(MuMIn)
LMERConvenienceFunctions::relLik(m.final, m.null)
LMERConvenienceFunctions::relLik(m.final, m.null)
m.1.slo <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial")
summary(m.1.slo, correlation=FALSE)
m.1.int <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial")
summary(m.1.int, correlation=FALSE)
m.1.slo <- glmer(SA_POS~1+SA_SEP*HAS_RC+(SA_POS|LEVEL1) + (SA_POS|LEVEL2), data=df, family="binomial")
summary(m.1.slo, correlation=FALSE)
)
(0
)))
# What about random slopes?
m.1.slo <- glmer(SA_POS~1+SA_SEP*HAS_RC+(0+SA_SEP*HAS_RC|LEVEL1) + (0+SA_SEP*HAS_RC|LEVEL2), data=df, family="binomial")
summary(m.1.slo, correlation=FALSE)
m.1.slo <- glmer(SA_POS~1+SA_SEP*HAS_RC+(0+SA_SEP+HAS_RC|LEVEL1) + (0+SA_SEP+HAS_RC|LEVEL2), data=df, family="binomial")
rm(list=ls(all=TRUE))
library(car); library(effects); library(lme4); library(MuMIn)
df <- read.csv("./204_assignment_brendel.csv", header = TRUE)
df$CENT <- factor(df$CENT)
## Exploration
summary(df)
hist(df$SA_SEP)
# Most are adjacent...
hist(sqrt(df$SA_SEP))
hist(sign(df$SA_SEP) * abs(df$SA_SEP)^(1/3))
hist(log(df$SA_SEP))
# These all are terrible. I can't imagine this will be a very informative factor.
plot(df$SA_POS)
# Many more instances of SA_POS = BEFORE than AFTER. This is consistent with Sapp (2019)'s claims that prenominal s치 is more frequent overall.
plot(df$SA_POS ~ df$CENT)
# Frequency of SA_POS = AFTER is much higher up to and including 15th century texts, whereafter it is consistently lower (except for potential outliers in 19th century?).
plot(df$SA_POS ~ df$HAS_RC)
# This looks like a fairly clear preference for SA_POS = AFTER when the NP contains a dependent relative clause, which seems to be what we'd expect.
plot(df$TEXT)
plot(df$SA_POS ~ df$TEXT)
# It looks like some texts contribute many more cases of an RC with s치 than other texts. Additionally, while most texts show SA_POS = AFTER is more frequent, they seem to differ greatly in the starkness of the difference in frequency between the two levels.
LEVEL1 <- df$CENT
LEVEL2 <- df$CENT:df$TEXT
m.1 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial")
summary(m.1, correlation=FALSE)
# Can CENT be deleted?
m.2 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL2), data=df, family="binomial")
anova(m.1, m.2)
# No; significant difference
# Can CENT/TEXT be deleted?
m.3 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1), data=df, family="binomial")
anova(m.1, m.3)
# No.
# Making model more readable now.
m.1r <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|CENT/TEXT), data=df, family="binomial")
drop1(m.1r, test="Chisq")
# Looks like SA_SEP:HAS_RC needs to stay in... unfortunately.
m.final <- m.1r
MuMIn::r.squaredGLMM(m.final)
# Well, the random effects help, but even the conditional R^2 is still not fantastic...
# # What about random slopes?
# m.1.slo <- glmer(SA_POS~1+SA_SEP*HAS_RC+(0+SA_SEP+HAS_RC|LEVEL1) + (0+SA_SEP+HAS_RC|LEVEL2), data=df, family="binomial")
# summary(m.1.slo, correlation=FALSE)
m.null <- glmer(SA_POS ~ 1 + (1|CENT/TEXT), family=binomial, data=df)
anova(m.final, m.null, test="Chisq")
# Still doing significantly better than the null model
C
LMERConvenienceFunctions::relLik(m.final, m.null)
x$PREDS.NUM.MM <- fitted(m.final) # or predict(model.final, type="response")
df$PREDS.NUM.MM <- fitted(m.final) # or predict(model.final, type="response")
df$PREDS.CAT.MM <- factor(ifelse( # make predictions: if
   df$PREDS.NUM.MM>=0.5,          # the pred. prob. of MATCH=="you" is >-0.5
   levels(df$SA_POS)[2],           # then predict "you"
   levels(df$SA_POS)[1]))          # otherwise predict "I"
table(df$SA_POS, df$PREDS.CAT.MM)
matrix(c("true negatives", "false negatives", "false positives", "true positives"),
       ncol=2, dimnames=list(DATA=levels(df$SA_POS), PREDICTIONS=sort(unique(df$PREDS.CAT.MM))))
matrix(c("true negatives", "false negatives", "false positives", "true positives"),
       ncol=2, dimnames=list(DATA=levels(df$SA_POS), PREDICTIONS=sort(unique(df$PREDS.CAT.MM))))
(5280+112) / length(df$PREDS.CAT.MM) # (tp+tn) / (tp+tn+fp+fn) = 0.6567624
5280/(5280+762) # tp/(tp+fp) = 0.639869
5280/(5280+65) # tp/(tp+fn) = 0.5366163
(baseline.1 <- max(prop.table(table(df$SA_POS))))   # 0.5515591
(baseline.2 <- sum(prop.table(table(df$SA_POS))^2)) # 0.5053167
sum(dbinom(sum(                               # compute the sum of binomial probabilities
   df$SA_POS==df$PREDS.CAT.MM):length(df$PREDS.CAT.MM), # making as many correct 'predictions' as you have or more
   length(df$PREDS.CAT.MM),                      # when making this many 'predictions'
   baseline.1)) # ***
sum(dbinom(sum(                               # compute the sum of binomial probabilities
   df$SA_POS==df$PREDS.CAT.MM):length(df$PREDS.CAT.MM), # making as many correct 'predictions' as you have or more
   length(df$PREDS.CAT.MM),                      # when making this many 'predictions'
   baseline.2)) # ***
?seq
plot(ia.sep <- effect("SA_POS:HAS_RC", m.final, type="response", grid=TRUE)
preds.hyp.sep <- data.frame(ia.sep$x, PREDICTIONS=ilogit(ia.sep$fit), LOWER=ilogit(ia.sep$lower), UPPER=ilogit(ia.sep$upper))
preds.hyp.sep.split <- split(preds.hyp.sep, preds.hyp.sep$TYPE); preds.hyp.sep.split
plot(ia.sep <- effect("SA_POS:HAS_RC", m.final, type="response", grid=TRUE)
)
plot(ia.sep <- effect("SA_POS:HAS_RC", m.final, type="response", grid=TRUE))
plot(ia.sep <- effect("SA_SEP:HAS_RC", m.final, type="response", grid=TRUE))
preds.hyp.sep <- data.frame(ia.sep$x, PREDICTIONS=ilogit(ia.sep$fit), LOWER=ilogit(ia.sep$lower), UPPER=ilogit(ia.sep$upper))
ilogit <- function (x) {
   1/(1 + exp(-x))
}
plot(ia.sep <- effect("SA_SEP:HAS_RC", m.final, type="response", grid=TRUE))
preds.hyp.sep <- data.frame(ia.sep$x, PREDICTIONS=ilogit(ia.sep$fit), LOWER=ilogit(ia.sep$lower), UPPER=ilogit(ia.sep$upper))
preds.hyp.sep.split <- split(preds.hyp.sep, preds.hyp.sep$TYPE); preds.hyp.sep.split
preds.hyp.sep.split <- split(preds.hyp.sep, preds.hyp.sep$HAS_RC); preds.hyp.sep.split
plot(ia.sep <- effect("SA_SEP:HAS_RC", m.final, type="response", grid=TRUE))
preds.hyp.sep <- data.frame(ia.sep$x, PREDICTIONS=ilogit(ia.sep$fit), LOWER=ilogit(ia.sep$lower), UPPER=ilogit(ia.sep$upper))
preds.hyp.sep.split <- split(preds.hyp.sep, preds.hyp.sep$HAS_RC); preds.hyp.sep.split
plot(ia.sep <- effect("SA_SEP:HAS_RC", m.final, type="response", xlevels=list(SA_SEP=seq(0, 4.5, length.out=19))), grid=TRUE))
plot(ia.sep <- effect("SA_SEP:HAS_RC", m.final, type="response", xlevels=list(SA_SEP=seq(0, 4.5, length.out=19)), grid=TRUE))
summary(df)
plot(ia.sep <- effect("SA_SEP:HAS_RC", m.final, type="response", xlevels=list(SA_SEP=seq(0, 5, length.out=5)), grid=TRUE))
plot(ia.sep <- effect("SA_SEP:HAS_RC", m.final, type="response", ylim=c(0, 1), grid=TRUE))
LEVEL1 <- df$CENT
LEVEL2 <- df$CENT:df$TEXT
m.1 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial", correlation=FALSE)
summary(m.1, correlation=FALSE)
# Can CENT be deleted?
m.2 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL2), data=df, family="binomial", correlation=FALSE)
anova(m.1, m.2)
# No; significant difference
# Can CENT/TEXT be deleted?
m.3 <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|LEVEL1), data=df, family="binomial", correlation=FALSE)
anova(m.1, m.3)
# No.
# Making model more readable now.
m.1r <- glmer(SA_POS~1+SA_SEP*HAS_RC+(1|CENT/TEXT), data=df, family="binomial", correlation=FALSE)
drop1(m.1r, test="Chisq")
# Looks like SA_SEP:HAS_RC needs to stay in... unfortunately.
m.final <- m.1r
MuMIn::r.squaredGLMM(m.final)
# Well, the random effects help, but even the conditional R^2 is still not fantastic...
m.null <- glmer(SA_POS ~ 1 + (1|CENT/TEXT), family=binomial, data=df)
anova(m.final, m.null, test="Chisq")
# Still doing significantly better than the null model
LMERConvenienceFunctions::relLik(m.final, m.null)
# compute the predicted probabilities of the highest interaction with the effects package and store them in preds.hyp.sep
plot(ia.sep <- effect("SA_SEP:HAS_RC", m.final, type="response", ylim=c(0, 1), grid=TRUE))
preds.hyp.sep <- data.frame(ia.sep$x, PREDICTIONS=ilogit(ia.sep$fit), LOWER=ilogit(ia.sep$lower), UPPER=ilogit(ia.sep$upper))
preds.hyp.sep.split <- split(preds.hyp.sep, preds.hyp.sep$HAS_RC); preds.hyp.sep.split
rm(list=ls(all=TRUE))
library(car); library(effects); library(lme4); library(MuMIn)
ilogit <- function (x) {
   1/(1 + exp(-x))
}
df <- read.csv("./204_assignment_brendel.csv", header = TRUE)
df$CENT <- factor(df$CENT)
## Exploration
summary(df)
# hist(df$SA_SEP)
# # Most are adjacent...
# hist(sqrt(df$SA_SEP))
# hist(sign(df$SA_SEP) * abs(df$SA_SEP)^(1/3))
# hist(log(df$SA_SEP))
# # These all are terrible. I can't imagine this will be a very informative factor.
plot(df$SA_POS)
# Many more instances of SA_POS = BEFORE than AFTER. This is consistent with Sapp (2019)'s claims that prenominal s치 is more frequent overall.
plot(df$SA_POS ~ df$CENT)
# Frequency of SA_POS = AFTER is much higher up to and including 15th century texts, whereafter it is consistently lower (except for potential outliers in 19th century?).
plot(df$SA_POS ~ df$HAS_RC)
# This looks like a fairly clear preference for SA_POS = AFTER when the NP contains a dependent relative clause, which seems to be what we'd expect.
plot(df$TEXT)
plot(df$SA_POS ~ df$TEXT)
# It looks like some texts contribute many more cases of an RC with s치 than other texts. Additionally, while most texts show SA_POS = AFTER is more frequent, they seem to differ greatly in the starkness of the difference in frequency between the two levels.
LEVEL1 <- df$CENT
LEVEL2 <- df$CENT:df$TEXT
m.1 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial")
summary(m.1, correlation=FALSE)
# Can CENT be deleted?
m.2 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL2), data=df, family="binomial")
anova(m.1, m.2)
# No; significant difference
# Can CENT/TEXT be deleted?
m.3 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL1), data=df, family="binomial")
anova(m.1, m.3)
# No.
# Making model more readable now.
m.1r <- glmer(SA_POS~1+HAS_RC+(1|CENT/TEXT), data=df, family="binomial")
drop1(m.1r, test="Chisq")
# Looks like SA_SEP:HAS_RC needs to stay in... unfortunately.
m.final <- m.1r
MuMIn::r.squaredGLMM(m.final)
m.null <- glmer(SA_POS ~ 1 + (1|CENT/TEXT), family=binomial, data=df)
anova(m.final, m.null, test="Chisq")
# Still doing significantly better than the null model
LMERConvenienceFunctions::relLik(m.final, m.null)
plot(df$SA_POS~df$HAS_RC)
table(df$SA_POS, df$HAS_RC)
plot(ia.sep <- effect("HAS_RC", m.final, type="response", ylim=c(0, 1), grid=TRUE))
preds.hyp.sep <- data.frame(ia.sep$x, PREDICTIONS=ilogit(ia.sep$fit), LOWER=ilogit(ia.sep$lower), UPPER=ilogit(ia.sep$upper))
preds.hyp.sep.split <- split(preds.hyp.sep, preds.hyp.sep$HAS_RC); preds.hyp.sep.splitplot(ia.sep <- effect("HAS_RC", m.final, type="response", grid=TRUE))
plot(ia.sep <- effect("HAS_RC", m.final, type="response", grid=TRUE))
plot(ia.sep <- effect("HAS_RC", m.final, type="response", grid=TRUE))
m.final
summary(m.1, correlation=FALSE)
MuMIn::r.squaredGLMM(m.final)
m.null <- glmer(SA_POS ~ 1 + (1|CENT/TEXT), family=binomial, data=df)
anova(m.final, m.null, test="Chisq")
baseline.1 <- max(prop.table(table(df$SA_POS)))   # 85.95%
baseline.2 <- sum(prop.table(table(df$SA_POS))^2) # 75.84%
baseline.1
baseline.2
MuMIn::r.squaredGLMM(m.final)
df$PREDS.NUM.MM <- fitted(m.final) # or predict(model.final, type="response")
df$PREDS.CAT.MM <- factor(ifelse( # make predictions: if
   df$PREDS.NUM.MM>=0.5,          # the pred. prob. of MATCH=="you" is >-0.5
   levels(df$SA_POS)[2],           # then predict "you"
   levels(df$SA_POS)[1]))          # otherwise predict "I"
table(df$SA_POS, df$PREDS.CAT.MM)
(5216+144) / length(df$PREDS.CAT.MM) # 86.70% accuracy; not bad!
matrix(c("true negatives", "false negatives", "false positives", "true positives"),
       ncol=2, dimnames=list(DATA=levels(df$SA_POS), PREDICTIONS=sort(unique(df$PREDS.CAT.MM))))
(5216+144) / length(df$PREDS.CAT.MM) # 86.19% accuracy; not bad!
5216/(5216+730) # 87.39%
5216/(5216+129) # 90.78%
baseline.1 <- max(prop.table(table(df$SA_POS)))   # 85.95%
baseline.1
baseline.2 <- sum(prop.table(table(df$SA_POS))^2) # 75.84%
baseline.2
(5216+144) / length(df$PREDS.CAT.MM) # 86.19% accuracy; not bad!
sum(dbinom(sum(                               # compute the sum of binomial probabilities
   df$SA_POS==df$PREDS.CAT.MM):length(df$PREDS.CAT.MM), # making as many correct 'predictions' as you have or more
   length(df$PREDS.CAT.MM),                      # when making this many 'predictions'
   baseline.1)) # ***
sum(dbinom(sum(                               # compute the sum of binomial probabilities
   df$SA_POS==df$PREDS.CAT.MM):length(df$PREDS.CAT.MM), # making as many correct 'predictions' as you have or more
   length(df$PREDS.CAT.MM),                      # when making this many 'predictions'
   baseline.2)) # ***                           and this is your baseline
plot(allEffects(m.final),
     ylab="Predicted probability of 'AFTER'", ylim=c(0,1),
     type="response", grid=TRUE)
levels(df$SA_POS)
plot(allEffects(m.final),
     ylab="Predicted probability of 'BEFORE'", ylim=c(0,1),
     type="response", grid=TRUE)
df$SA_POS <- relevel(df$SA_POS, 'BEFORE')
df$SA_POS
levels(df$SA_POS)
df <- read.csv("./204_assignment_brendel.csv", header = TRUE)
df$CENT <- factor(df$CENT)
df$SA_POS <- relevel(df$SA_POS, 'BEFORE')
## Exploration
summary(df)
# hist(df$SA_SEP)
# # Most are adjacent...
# hist(sqrt(df$SA_SEP))
# hist(sign(df$SA_SEP) * abs(df$SA_SEP)^(1/3))
# hist(log(df$SA_SEP))
# # These all are terrible. I can't imagine this will be a very informative factor.
plot(df$SA_POS)
# Many more instances of SA_POS = BEFORE than AFTER. This is consistent with Sapp (2019)'s claims that prenominal s치 is more frequent overall.
LEVEL1 <- df$CENT
LEVEL2 <- df$CENT:df$TEXT
m.1 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL1) + (1|LEVEL2), data=df, family="binomial")
summary(m.1, correlation=FALSE)
# Can CENT be deleted?
m.2 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL2), data=df, family="binomial")
anova(m.1, m.2)
# No; significant difference
# Can CENT/TEXT be deleted?
m.3 <- glmer(SA_POS~1+HAS_RC+(1|LEVEL1), data=df, family="binomial")
anova(m.1, m.3)
# No.
# Making model more readable now.
m.1r <- glmer(SA_POS~1+HAS_RC+(1|CENT/TEXT), data=df, family="binomial")
m.final <- m.1r
MuMIn::r.squaredGLMM(m.final)
drop1(m.final, test='II')
drop1(m.final, test='Chisq')
m.null <- glmer(SA_POS ~ 1 + (1|CENT/TEXT), family=binomial, data=df)
anova(m.final, m.null, test="Chisq")
plot(ia.sep <- effect("HAS_RC", m.final, type="response", grid=TRUE))
preds.hyp.sep <- data.frame(ia.sep$x, PREDICTIONS=ilogit(ia.sep$fit), LOWER=ilogit(ia.sep$lower), UPPER=ilogit(ia.sep$upper))
preds.hyp.sep.split <- split(preds.hyp.sep, preds.hyp.sep$HAS_RC); preds.hyp.sep.splitplot(lm(SA_POS ~ HAS_RC, data=df))
plot(lm(SA_POS ~ HAS_RC, data=df))
summary(lm(SA_POS ~ HAS_RC, data=df))
summary(glm(SA_POS ~ HAS_RC, data=df))
summary(lm(SA_POS ~ HAS_RC, data=df))
summary(glm(SA_POS ~ HAS_RC, data=df))
